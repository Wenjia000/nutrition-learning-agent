{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgxkrZe9vFUMYO6l38E/zJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wenjia000/nutrition-learning-agent/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "vw5TIR_wAszc",
        "outputId": "1b15cb7b-1f2d-42a3-8814-013622caed95"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error: invalid key: inooe000@gmail.com\n",
            "error: key does not contain a section: Wenjia000\n",
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "error: open(\"drive/MyDrive/To-do list.gsheet\"): Operation not supported\n",
            "error: unable to index file 'drive/MyDrive/To-do list.gsheet'\n",
            "fatal: adding files failed\n",
            "error: switch `m' requires a value\n",
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Wenjia000/nutrition-learning-agent.git'\n",
            "\u001b[mDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4096282256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m }\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3051\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2631\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2633\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2634\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4096282256.py\u001b[0m in \u001b[0;36mcollect_answer\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     ans = input(\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;34mf\"Your answer:\\n{state['last_question']}\\n(type 'hint' or 'stop')\\n> \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     ).strip().lower()\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q langgraph langchain-openai scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Optional, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# load API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7,\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "# load data\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/USDA.csv')\n",
        "\n",
        "\n",
        "# make food name shorter\n",
        "def simplify_name(desc: str) -> str:\n",
        "    parts = desc.split(\",\")\n",
        "    if len(parts) >= 2:\n",
        "        return parts[0].lower().strip() + \" \" + parts[1].lower().strip()\n",
        "    return parts[0].lower().strip()\n",
        "\n",
        "\n",
        "# question generators(easy, meduim, hard)\n",
        "\n",
        "NUTRIENTS_EASY = [\"Protein\", \"Calories\", \"TotalFat\", \"Carbohydrate\", \"Sugar\", \"Calcium\"]\n",
        "\n",
        "GOALS_MEDIUM = {\n",
        "    \"build muscle\": \"Protein\",\n",
        "    \"bone health\": \"Calcium\",\n",
        "    \"a low-fat diet\": \"TotalFat\",\n",
        "    \"a low-sugar diet\": \"Sugar\",\n",
        "    \"heart health\": \"Sodium\",\n",
        "}\n",
        "\n",
        "def generate_easy_question(df):\n",
        "    sampled = df.sample(2)\n",
        "    f1, f2 = sampled.iloc[0], sampled.iloc[1]\n",
        "    name1 = simplify_name(f1[\"Description\"])\n",
        "    name2 = simplify_name(f2[\"Description\"])\n",
        "    nutrient = random.choice(NUTRIENTS_EASY)\n",
        "    question = f\"Which has more {nutrient.lower()}: {name1} or {name2}?\"\n",
        "    answer = name1 if f1[nutrient] > f2[nutrient] else name2\n",
        "    return {\"question\": question, \"answer\": answer, \"difficulty\": \"easy\"}\n",
        "\n",
        "\n",
        "def generate_medium_question(df):\n",
        "    sampled = df.sample(2)\n",
        "    f1, f2 = sampled.iloc[0], sampled.iloc[1]\n",
        "    name1 = simplify_name(f1[\"Description\"])\n",
        "    name2 = simplify_name(f2[\"Description\"])\n",
        "    goal, nutrient = random.choice(list(GOALS_MEDIUM.items()))\n",
        "    v1, v2 = f1[nutrient], f2[nutrient]\n",
        "    question = f\"If you're trying to {goal}, which is better: {name1} or {name2}?\"\n",
        "    if \"low\" in goal:\n",
        "        answer = name1 if v1 < v2 else name2\n",
        "    else:\n",
        "        answer = name1 if v1 > v2 else name2\n",
        "    return {\"question\": question, \"answer\": answer, \"difficulty\": \"medium\"}\n",
        "\n",
        "\n",
        "def generate_hard_question(df):\n",
        "    sampled = df.sample(3)\n",
        "    f1, f2, f3 = sampled.iloc[0], sampled.iloc[1], sampled.iloc[2]\n",
        "    name1 = simplify_name(f1[\"Description\"])\n",
        "    name2 = simplify_name(f2[\"Description\"])\n",
        "    name3 = simplify_name(f3[\"Description\"])\n",
        "    question = f\"You want high protein but low fat. Which is best: {name1}, {name2}, or {name3}?\"\n",
        "    def score(food):\n",
        "        return food[\"Protein\"] - food[\"TotalFat\"]\n",
        "    scores = {name1: score(f1), name2: score(f2), name3: score(f3)}\n",
        "    answer = max(scores, key=scores.get)\n",
        "    return {\"question\": question, \"answer\": answer, \"difficulty\": \"hard\"}\n",
        "\n",
        "\n",
        "# logistic regression classifier\n",
        "np.random.seed(0)\n",
        "\n",
        "data = {\n",
        "    \"accuracy\": np.random.rand(500),\n",
        "    \"easy_correct\": np.random.randint(0, 5, 500),\n",
        "    \"medium_correct\": np.random.randint(0, 5, 500),\n",
        "    \"hard_correct\": np.random.randint(0, 5, 500),\n",
        "}\n",
        "df_sim = pd.DataFrame(data)\n",
        "\n",
        "df_sim[\"level\"] = pd.cut(\n",
        "    df_sim[\"accuracy\"],\n",
        "    bins=[0, 0.4, 0.7, 1.0],\n",
        "    labels=[0, 1, 2]\n",
        ").astype(int)\n",
        "\n",
        "X = df_sim[[\"accuracy\", \"easy_correct\", \"medium_correct\", \"hard_correct\"]]\n",
        "y = df_sim[\"level\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# agent state\n",
        "class AgentState(TypedDict):\n",
        "    last_question: Optional[str]\n",
        "    last_answer: Optional[str]\n",
        "    last_difficulty: Optional[str]\n",
        "    user_answer: Optional[str]\n",
        "    history: List[str]\n",
        "    correct_list: List[int]\n",
        "    accuracy: float\n",
        "    easy_correct: int\n",
        "    medium_correct: int\n",
        "    hard_correct: int\n",
        "    predicted_level: Optional[int]\n",
        "    continue_flag: bool\n",
        "    ab_version: str\n",
        "\n",
        "\n",
        "# nodes\n",
        "\n",
        "# randomly assign A/B version\n",
        "def assign_ab_version(state: AgentState):\n",
        "    state[\"ab_version\"] = random.choice([\"A\", \"B\"])\n",
        "    state[\"history\"].append(f\"AB version = {state['ab_version']}\")\n",
        "    return state\n",
        "\n",
        "# greeting node\n",
        "def greeting(state: AgentState):\n",
        "    state[\"history\"].append(\"Hello! Let's start your nutrition learning session.\")\n",
        "    if state[\"predicted_level\"] is None:\n",
        "        state[\"predicted_level\"] = 0\n",
        "    return state\n",
        "\n",
        "# choose difficulty and generate question\n",
        "def ask_question(state: AgentState):\n",
        "    lvl = state[\"predicted_level\"]\n",
        "    if lvl == 0:\n",
        "        qa = generate_easy_question(df)\n",
        "    elif lvl == 1:\n",
        "        qa = generate_medium_question(df)\n",
        "    else:\n",
        "        qa = generate_hard_question(df)\n",
        "    state[\"last_question\"] = qa[\"question\"]\n",
        "    state[\"last_answer\"] = qa[\"answer\"]\n",
        "    state[\"last_difficulty\"] = qa[\"difficulty\"]\n",
        "    state[\"history\"].append(\"QUESTION: \" + qa[\"question\"])\n",
        "    return state\n",
        "\n",
        "# tutor node using A/B difference\n",
        "def llm_node(state: AgentState):\n",
        "    q = state[\"last_question\"]\n",
        "    if state[\"ab_version\"] == \"A\":\n",
        "        prompt = f\"Encourage the student to answer: {q}\"\n",
        "    else:\n",
        "        prompt = f\"Give a step-by-step friendly explanation and encourage answering:\\n{q}\"\n",
        "    reply = llm.invoke(prompt).content\n",
        "    state[\"history\"].append(\"TUTOR: \" + reply)\n",
        "    return state\n",
        "\n",
        "\n",
        "def collect_answer(state: AgentState):\n",
        "    ans = input(\n",
        "        f\"Your answer:\\n{state['last_question']}\\n(type 'hint' or 'stop')\\n> \"\n",
        "    ).strip().lower()\n",
        "    state[\"user_answer\"] = ans\n",
        "    return state\n",
        "\n",
        "# route: hint / grade / end\n",
        "def route_user_action(state: AgentState):\n",
        "    if state[\"user_answer\"] == \"hint\":\n",
        "        return \"hint\"\n",
        "    if state[\"user_answer\"] == \"stop\":\n",
        "        return \"end\"\n",
        "    return \"grade\"\n",
        "\n",
        "# hint node with A/B variation\n",
        "def hint_node(state: AgentState):\n",
        "    q = state[\"last_question\"]\n",
        "    diff = state[\"last_difficulty\"]\n",
        "    if state[\"ab_version\"] == \"A\":\n",
        "        prompt = f\"Give a short hint:\\n{q}\"\n",
        "    else:\n",
        "        prompt = f\"Give a detailed reasoning-style hint for this {diff} question:\\n{q}\"\n",
        "    msg = llm.invoke(prompt).content\n",
        "    state[\"history\"].append(\"HINT: \" + msg)\n",
        "    print(\"\\n[HINT] \" + msg + \"\\n\")\n",
        "    return state\n",
        "\n",
        "# grade answer\n",
        "def grade_answer(state: AgentState):\n",
        "    user = state[\"user_answer\"]\n",
        "    correct = state[\"last_answer\"]\n",
        "    diff = state[\"last_difficulty\"]\n",
        "\n",
        "    if user is None:\n",
        "        return state\n",
        "\n",
        "    is_correct = 1 if user == correct else 0\n",
        "    state[\"correct_list\"].append(is_correct)\n",
        "\n",
        "    if is_correct:\n",
        "        if diff == \"easy\": state[\"easy_correct\"] += 1\n",
        "        if diff == \"medium\": state[\"medium_correct\"] += 1\n",
        "        if diff == \"hard\": state[\"hard_correct\"] += 1\n",
        "\n",
        "    if len(state[\"correct_list\"]) >= 5:\n",
        "        state[\"accuracy\"] = sum(state[\"correct_list\"][-5:]) / 5\n",
        "\n",
        "    print(f\"\\nRESULT: {'correct' if is_correct else 'incorrect'}\\n\")\n",
        "    return state\n",
        "\n",
        "# classifier update\n",
        "def classify_level(state: AgentState):\n",
        "    user_X = pd.DataFrame([{\n",
        "        \"accuracy\": state[\"accuracy\"],\n",
        "        \"easy_correct\": state[\"easy_correct\"],\n",
        "        \"medium_correct\": state[\"medium_correct\"],\n",
        "        \"hard_correct\": state[\"hard_correct\"],\n",
        "    }])\n",
        "    lvl = int(clf.predict(user_X)[0])\n",
        "    state[\"predicted_level\"] = lvl\n",
        "    print(\"LEVEL =\", lvl)\n",
        "    return state\n",
        "\n",
        "# continue rule\n",
        "def continue_or_end(state: AgentState):\n",
        "    if len(state[\"correct_list\"]) >= 10:\n",
        "        state[\"continue_flag\"] = False\n",
        "    return state\n",
        "\n",
        "\n",
        "# build graph\n",
        "# Flow:\n",
        "# assign_ab\n",
        "#   → greeting\n",
        "#       → ask_question\n",
        "#           → llm_talk\n",
        "#               → collect_answer\n",
        "#                   → hint → collect_answer\n",
        "#                   → grade → continue_check → classify → ask_question\n",
        "#                   → end\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"assign_ab\", assign_ab_version)\n",
        "graph.add_node(\"greeting\", greeting)\n",
        "graph.add_node(\"ask_question\", ask_question)\n",
        "graph.add_node(\"llm_talk\", llm_node)\n",
        "graph.add_node(\"collect_answer\", collect_answer)\n",
        "graph.add_node(\"hint\", hint_node)\n",
        "graph.add_node(\"grade\", grade_answer)\n",
        "graph.add_node(\"classify\", classify_level)\n",
        "graph.add_node(\"continue_check\", continue_or_end)\n",
        "\n",
        "graph.set_entry_point(\"assign_ab\")\n",
        "graph.add_edge(\"assign_ab\", \"greeting\")\n",
        "graph.add_edge(\"greeting\", \"ask_question\")\n",
        "graph.add_edge(\"ask_question\", \"llm_talk\")\n",
        "graph.add_edge(\"llm_talk\", \"collect_answer\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"collect_answer\",\n",
        "    route_user_action,\n",
        "    {\n",
        "        \"hint\": \"hint\",\n",
        "        \"grade\": \"grade\",\n",
        "        \"end\": \"__end__\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"hint\", \"collect_answer\")\n",
        "graph.add_edge(\"grade\", \"continue_check\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"continue_check\",\n",
        "    lambda s: \"continue\" if s[\"continue_flag\"] else \"end\",\n",
        "    {\n",
        "        \"continue\": \"classify\",\n",
        "        \"end\": \"__end__\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"classify\", \"ask_question\")\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "initial_state: AgentState = {\n",
        "    \"last_question\": None,\n",
        "    \"last_answer\": None,\n",
        "    \"last_difficulty\": None,\n",
        "    \"user_answer\": None,\n",
        "    \"history\": [],\n",
        "    \"correct_list\": [],\n",
        "    \"accuracy\": 0.0,\n",
        "    \"easy_correct\": 0,\n",
        "    \"medium_correct\": 0,\n",
        "    \"hard_correct\": 0,\n",
        "    \"predicted_level\": None,\n",
        "    \"continue_flag\": True,\n",
        "    \"ab_version\": \"A\",\n",
        "}\n",
        "\n",
        "final_state = app.invoke(initial_state)"
      ]
    }
  ]
}